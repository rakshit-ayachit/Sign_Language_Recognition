# Sign Language Recognition using Live Webcam Feed and LSTM Deep Learning

This project aims to develop a sign language recognition system using live webcam feed, OpenCV (cv2), and LSTM (Long Short-Term Memory) deep learning architecture. The goal is to enable accurate and real-time recognition of sign language gestures, providing an accessible communication tool for individuals with hearing impairments.

## Introduction

Sign language is an essential means of communication for the deaf and hard of hearing community. This project combines computer vision and deep learning to bridge the communication gap. The system captures hand gestures through the webcam feed, processes them using OpenCV and Mediapipe to extract hand landmarks, and passes the sequence of landmarks through an LSTM model for recognition.
